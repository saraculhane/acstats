---
output:
  pdf_document: default
  html_document: default
---
<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Computation and Application to Dataset




## Dataset 1: Walmart Store Data from Kaggle

From a 2014 Kaggle competition with a goal of forecasting Walmart Sales, the Walmart Store dataset provides information on sales for $n=45$ located in different regions of the United States.  Each store has Weekly Sales data from $2010-02-05$ to $2012-11-01$, information by department and a binary IsHoliday variable. Evidently, this data has fairly limited number of predictors but was one of few available retail datasets, thus was used as a warm up application before applying to a larger dataset.

[@walmartdata]

### Prior Analysis : Kimmo Kiviluoto and Erkki Oja

In 1998, Kiviluoto and Erkki Oja used ICA on parallel Financial Time Series from a retail chain of 40 stores.  They claim in their paper \textit{Independent Component Analysis for Parallel Financial Time Series} that by removing the fundamental factors of the data, they were able to see the impact of management decisions of a particular store more clearly.  [pg. 2,@kimmo1998]

With the Walmart store data, an attempt to show a similar, interesting result using retail data will be made.  However, this will look at Weekly Sales not Cash-flow data, which will probably not generate as robust results since cash-flow provides more information about financial activities of a store than just sales.


### Exploration 

```{r, include=FALSE}
library(readr)
library(lubridate)
library(fastICA)
library(tidyverse)
library(ica)
library(mosaic)
library(xtable)
library(dplyr)
library(knitr)
walmart <- read_csv("waltrain.csv")
head(walmart)
w2 <- data.frame()
w2 <- walmart %>% select(Store, Weekly_Sales)%>% 
  group_by(Store) %>% 
  summarise(
    n = n(),
    Weekly_Sales = mean(Weekly_Sales, na.rm = TRUE)
  )
```




## FastICA Application

```{r,message=FALSE}
# Set nc= 3 based on dimensions
a <-fastICA(w2,3, alg.typ="parallel",fun= "logcosh",row.norm=FALSE,maxit=5,tol= 0.0001,verbose=TRUE)
par(mfrow = c(1, 3))
plot(a$X, main = "Pre-processed data")
plot(a$X %*% a$K, main = "PCA components" )
plot(a$S, main = "fastICA components")  
```



From these plots, there appears to be some clustering uncovered by the fastICA algorithm. One possibility is that the cluster of 8 stores at towards the top represents the most productive in terms of sales but this could just be random noise.  In general,the dataset appears to be too small in terms of both dimensions and observations to come to any explicit conclusions about underlying influences of Weekly Sales and top performing stores.


### Dataset 2: Sample Superstore data 

The Superstore Sample data appears in the December Tableau User Group presentation. 

Since this data has higher dimensionality and more variety of variables, the ICA model should be more effective here. Unlike with the prior simulations, only the fastICA algorithm will be used. [@storedata]   


Note: Geographic locations have been altered to include Canadian locations numerically (provinces / regions). \newline

```{r,include=FALSE,warning=FALSE}
store <- read_csv("superstore.csv")
```


### Select Variables of Interest

#### Sample of Variables

```{r, echo=FALSE}
colnames(store) %>% broom::tidy()
```

#### By Province

```{r}
s <- store %>% 
  select(`Order ID`,`Row ID`,`Order Date`,`Order Quantity`,Sales, Discount, Profit,`Unit Price`, `Shipping Cost`,Province) %>%
  group_by(Province) %>% summarise(n = n(),
    Order_Q = sum(`Order Quantity`),
    Sales = sum(Sales),
    Discount = sum(Discount),
    Profit = sum(Profit),
    U_Price = sum(`Unit Price`),
    Ship_C = sum(`Shipping Cost`))

x <- s[,-1] # Remove Province variables to fit ICA model (non-numeric)
```

```{r, message=FALSE}
t <- fastICA(x,4, alg.typ="parallel",fun= "logcosh",row.norm=FALSE,maxit=5,tol= 0.0001,verbose=TRUE)
```

```{r}
par(mfrow = c(1, 3))
plot(t$X, main = "Pre-processed data")
plot(t$X %*% t$K, main = "PCA components" )
plot(t$S, main = "ICA components")  
```

Looking at the superstore data by Province, some limited clustering exists that could potentially indicate nuances in store performance by Province or more volume of orders.   Unfortunately, this view does not seem to be robust enough for any substantial conclusions.


#### By Order ID

```{r}
z <- store %>% 
  select(`Order ID`,`Row ID`,`Order Date`,`Order Quantity`,Sales, Discount, Profit,`Unit Price`, `Shipping Cost`,Province) %>%
  group_by(`Order ID`) %>% summarise(
  n = n(),
    Order_Q = sum(`Order Quantity`),
    Sales = sum(Sales),
    Discount = sum(Discount),
    Profit = sum(Profit),
    U_Price = sum(`Unit Price`),
    Ship_C = sum(`Shipping Cost`))
z1 <- z[sample(1000),] #take sub sample
```



```{r, message=FALSE}
test <- fastICA(z1,8, alg.typ="parallel",fun= "logcosh",row.norm=FALSE,maxit=5,tol= 0.0001,verbose=TRUE)
par(mfrow = c(1, 3))
plot(test$X, main = "Pre-processed data")
plot(test$X %*% test$K, main = "PCA components" )
plot(test$S, main = "ICA components")  
```

While the PCA model provides minimal information, the ICA model for the superstore data by order ID appears to have separated some particular ID's from a massive cluster of orders.  These orders could be the most atypical in terms of quantity or price, but this separation indicates the effectiveness of the ICA model on the data in general. 


### Run 1000 fastICA Iterations

```{r, results="hide", warning=FALSE}
set.seed(10)
N <- 1000
itF <- rep(NA,1,N)

colMax <- function(data) sapply(data, max, na.rm = TRUE)[1] 
gen_s <- function(N,S)  {
  for (i in 1:N) {
    test <- fastICA(z1,8, alg.typ="parallel",fun= "logcosh",row.norm=FALSE,maxit=5,tol= 0.0001,verbose=TRUE)

    itF[i] <- colMax(abs(congru(S,test$S)))


  }
  return(itF)
}
```

```{r,include=FALSE}
reps <- gen_s(N,z1)
```

### Absolute Maxmimum Congruency Coefficient

```{r}
kable(favstats(reps))
```

Like with the CPP simulation, the superstore data had to be compared to the observations as opposed to the true S.  Unsurprisingly, this produces a very low coefficient with high standard deviation so does not seem inform the analysis in a particularly useful way.  

Largely, these two dataset applications may have not been the most compelling examples of the power of ICA due to their limited dimensionality and lack of complexity to their data structure as a whole.

